{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Training BERT Model on Multi-Label Classification in English</h1>\n",
    "We load the data and fine-tune a BERT model on the tasks of predicting multiple labels on the current or next utterance.\n",
    "\n",
    "\n",
    "<i>vers. 10/2023</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Data Preprocessing & Model Initialisation </h3>\n",
    "\n",
    "First we load the appropriate dataset, process it into the proper format and initialise the model we want to fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import cuda, device\n",
    "import torch\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose model size and task\n",
    "\n",
    "task = 'next'  # \"current\" #  predict labels for current / next utterance\n",
    "\n",
    "model_size = 'large'  # \"base\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD THE RIGHT DATA AND GET THE MODEL NAME, OUTPUT PATH\n",
    "dataset_name = 'carglass'\n",
    "\n",
    "if task == \"current\":\n",
    "    dataset = load_dataset(\"csv\", data_files={\"train\":\"/data/daily_dialog_train_ohe.csv\", 'validation':'data/daily_dialog_val_ohe.csv', \"test\": \"/data/daily_dialog_test_ohe.csv\"})\n",
    "    output_path = \"\"\n",
    "\n",
    "    if model_size == 'large':\n",
    "        model_name = 'bert-large-cased'\n",
    "        output_path = \".../Filter Rerank/Bert_Large_Current\"\n",
    "\n",
    "\n",
    "    else:\n",
    "        model_name = 'bert-base-cased'\n",
    "        output_path = \".../Filter Rerank/Bert_Base_Current\"\n",
    "\n",
    "        \n",
    "else:\n",
    "    dataset = load_dataset(\"csv\", data_files={\"train\":\"/data/daily_dialog_train_next_ohe.csv\", 'validation':'/data/daily_dialog_val_next_ohe.csv', \"test\": \"/data/daily_dialog_test_next_ohe.csv\"})\n",
    "    output_path = \"path/to/model\"\n",
    "\n",
    "    if model_size == 'large':\n",
    "        model_name = 'bert-large-cased'\n",
    "        output_path = \"Bert_Large_Next\"\n",
    "\n",
    "\n",
    "    else:\n",
    "        model_name = 'bert-base-cased'\n",
    "        output_path = \"Bert_Base_Next\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    os.makedirs(output_path + '/results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACTIVATE CUDA\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PEEK AT THE DATA\n",
    "example = dataset['train'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET LABELS\n",
    "\n",
    "labels = [label for label in dataset['train'].features.keys() if label not in ['input']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD TOKENIZER\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"input\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESS THE DATASETS\n",
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)\n",
    "\n",
    "example = encoded_dataset['train'][0]\n",
    "print(example.keys())\n",
    "\n",
    "tokenizer.decode(example['input_ids'])\n",
    "\n",
    "#example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET THE FORMAT AS PYTORCH TENSORS, TO OBTAIN PYTORCH DATASETS\n",
    "\n",
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD THE MODEL\n",
    "#Setting `problem_type` to be \"multi_label_classification\" makes sure we use the appropriate loss function, BCEWithLogitsLoss\n",
    "#The output layer has `len(labels)` output neurons, and we set the id2label and label2id mappings.\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, \n",
    "                                                        problem_type=\"multi_label_classification\", \n",
    "                                                        num_labels=len(labels),\n",
    "                                                        id2label=id2label,\n",
    "                                                        label2id=label2id,\n",
    "                                                        hidden_dropout_prob=0.5)\n",
    "\n",
    "#check the device\n",
    "device = torch.device(0)\n",
    "model = model.to(device)\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training Arguments and Metrics</h3>\n",
    "We then set the training hyper-parameters such as batch-size or number of epochs, and then we define the metrics we will compute during training and evaluation of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET THE ARGUMENTS AND HYPER-PARAMETERS\n",
    "\n",
    "batch_size = 16\n",
    "metric_name = \"f1\"\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_path,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=20,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps = 2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    save_total_limit = 2,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START WANDB SESSION\n",
    "\n",
    "wandb.init(\n",
    "    project=\"bert\",\n",
    "    config={\n",
    "        \"per_device_train_batch_size\": args.per_device_train_batch_size,\n",
    "        \"learning_rate\": args.learning_rate,\n",
    "        \"dataset\": dataset_name,\n",
    "    },\n",
    ")\n",
    "\n",
    "wandb_run.name = \"run_\" + \"bert\" + \"_\" + model_size + '_' + task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MULTILABEL METRICS\n",
    "# while training, we need to define a `compute_metrics` function, that returns a dictionary with the desired metric values\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "            'roc_auc': roc_auc,\n",
    "            'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verification\n",
    "\n",
    "print(encoded_dataset['train'][0]['labels'].type())\n",
    "\n",
    "print(encoded_dataset['train']['input_ids'][0])\n",
    "\n",
    "outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0).to(device), labels=encoded_dataset['train'][0]['labels'].unsqueeze(0).to(device))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training</h3>\n",
    "Time to train and evaluate the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise trainer module\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATION ON VALIDATION SET\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test</h3>\n",
    "We test the newly trained model on a sentence and peek into the outputs and how to transform the raw logits into actual predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test on a sentence\n",
    "\n",
    "text = \"allez-y madame je vous Ã©coute, que puis-je faire pour vous?\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "outputs = trainer.model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PEEK INTO THE OUTPUT\n",
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRINT LOGITS\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TURN LOGITS INTO PREDICTIONS AND LABELS\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prediction and Inference</h3>\n",
    "\n",
    "Now that our model has been fine-tuned, we can use it to predict labels on brand new data it has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PEEK INTO TEST DATA\n",
    "\n",
    "test = dataset['test'][0]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD THE MODEL\n",
    "#Setting `problem_type` to be \"multi_label_classification\" makes sure we use the appropriate loss function, BCEWithLogitsLoss\n",
    "#The output layer has `len(labels)` output neurons, and we set the id2label and label2id mappings.\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "LOAD_MODEL = False\n",
    "\n",
    "if LOAD_MODEM:\n",
    "    model_name_or_path = '/path/to/model'\n",
    "    config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "    id2label = [config.id2label[key] for key in sorted(config.id2label.keys(), key=lambda t: int(t))]\n",
    "    id2label = np.asarray(id2label)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\n",
    "\n",
    "else:\n",
    "    model = trainer.model\n",
    "\n",
    "device = torch.device(0)\n",
    "model = model.to(device)\n",
    "t = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(text, k=None, verbose=False):\n",
    "\n",
    "    features = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    features = features.to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**features)\n",
    "        logits = outputs[0]\n",
    "        logits = logits.sigmoid()\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    # sort rst by desceding order\n",
    "    pred_scores = np.sort(logits)[:, ::-1]\n",
    "    pred_ids = np.argsort(logits)[:, ::-1]\n",
    "\n",
    "    pred_scores = pred_scores[0]\n",
    "    pred_labels = id2label[pred_ids[0]]\n",
    "    \n",
    "    if k is not None:\n",
    "        pred_scores = pred_scores[:k]\n",
    "        pred_labels = pred_labels[:k]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'\"{text}\"')\n",
    "        for i, (s, l) in enumerate(zip(pred_scores, pred_labels)):\n",
    "            print(f\"{l:30} : {s}\")\n",
    "        print()\n",
    "    \n",
    "    return pred_labels, pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_set(test_data, k=None, verbose=False):\n",
    "  results_dic = {'input':[],'reference':[]} \n",
    "  for label in id2label:\n",
    "    results_dic[label] = []\n",
    "  for data in tqdm(test_data):\n",
    "      text = data['input']\n",
    "      results_dic['input'].append(text)\n",
    "      results_dic['reference'].append('<SEP>'.join([x for x in id2label if data[x] == 1]))\n",
    "      pred_label, pred_score = predict_sentence(text, k, verbose)\n",
    "      pred_label = list(pred_label)\n",
    "      for label in id2label:\n",
    "          idx = pred_label.index(label)\n",
    "          results_dic[label].append(pred_score[idx])\n",
    "  return results_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predict_set(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results['reference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE RESULTS\n",
    "pd.DataFrame(results).to_csv(output_path+'/results/results_socemo_multilabel_window3.csv', encoding = 'UTF-8', index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
