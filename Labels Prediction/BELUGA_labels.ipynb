{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Training Beluga (Llama2 model fine-tuned on Orca Dataset) on Multi-Label Sequence Generation in English</h1>\n",
    "We load the data and prompt Beluga to generate a sequence of labels associated with the next utterance.\n",
    "\n",
    "\n",
    "<i>vers. 10/2023</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET MODEL AND TOKENIZER\n",
    "model_name= \"stabilityai/StableBeluga-13B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "device = torch.device(0)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT PATH\n",
    "output_path = \"/Beluga\" \n",
    "\n",
    "import os\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET DATA\n",
    "path= \"/data/daily_dialog_test_next_window3.csv\"\n",
    "test = pd.read_csv(path, encoding = \"UTF-8\")\n",
    "\n",
    "verbatims = test['text']\n",
    "labels = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLE: MAKE PROMPT\n",
    "\n",
    "message = \"We consider the following labels: 'inform', 'question', 'directive', 'commissive', 'neutral', 'anger', 'disgust', 'fear', 'happiness', 'sadness' and 'surprise'. \\n Predict the sequence of labels associated with the utterance that follows the given dialogue. \\n\\n For example, Dialogue: 'Good morning , sir . Is there a bank near here ?\\nLabels: 'inform'. What labels are associated with the utterance following this dialogue: Dialogue: \" + verbatims[10]\n",
    "#prompt = f\"{system_prompt}### User: {message}\\n\\n### Assistant: \"\n",
    "inputs = tokenizer(message, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print(verbatims[10])\n",
    "print(labels[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE: GENERATE THE SEQUENCE\n",
    "output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(response[len(message):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION THAT TAKES THE CONTEXT AND TRANSFORMS IT INTO THE PROMPT FOR THE MODEL\n",
    "\n",
    "def make_prompt(element, k = None):    \n",
    "    if k is not None: #NO_CD: without conditioning on the length of the expected sequence of labels\n",
    "        prompt = \"Generate the response following the given context. For example: Contex: '\" + element\n",
    "\n",
    "    else: # CD: we condition by restraining the sequence generation on its expected lenght\n",
    "        if k == 1:\n",
    "            prompt = \"Predict the label associated with the utterance that follows the given dialogue.\\nWe consider the following labels: 'inform', 'question', 'directive', 'commissive', 'neutral', 'anger', 'disgust', 'fear', 'happiness', 'sadness' and 'surprise'. The answer must be one or a sequence of multiple labels from this list.\\n\\n\\Here are a few examples,\\nDialogue: 'Good morning , sir . Is there a bank near here ?\\nLabels: 'inform'.\\n\\n Dialogue: 'Is it far ?'\\nLabels:'inform'\\n Diloague: 'No , It's only about five minutes walk.'\\nLabels: 'inform', 'happiness'.\\n\\n\\nWhat label is associated with the utterance following this dialogue: Dialogue: \" + element\n",
    "\n",
    "        else:\n",
    "            prompt = \"Predict the sequence of \" + str(k) + \" labels associated with the utterance that follows the given dialogue.\\nWe consider the following labels: 'inform', 'question', 'directive', 'commissive', 'neutral', 'anger', 'disgust', 'fear', 'happiness', 'sadness' and 'surprise'. The answer must be one or a sequence of multiple labels from this list.\\n\\Here are a few examples,\\nDialogue: 'Good morning , sir . Is there a bank near here ?\\nLabels: 'inform'.\\n\\n Dialogue: 'Is it far ?'\\nLabels:'inform'\\n Diloague: 'No , It's only about five minutes walk.'\\nLabels: 'inform', 'happiness'.\\n\\n\\nWhat \" + str(k) + \" labels are associated with the utterance following this dialogue: Dialogue: \" + element\n",
    "    \n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENERATION LOOP, ITERATES OVER EVERY TEST SAMPLE TO GENERATE THE EXPECTED LABELS SEQUENCE\n",
    "#IN THE SAME LOOP WE GENERATE THE NO-CD LABELS AND THE CD LABELS\n",
    "\n",
    "from tqdm import tqdm\n",
    "responses = []\n",
    "responses_k = []\n",
    "\n",
    "for i in tqdm(range(len(verbatims))):\n",
    "    input = verbatims[i].split('<SEP>')\n",
    "    input = 'SPEAKER A: ' + input [0] + ' SPEAKER B: ' + input[1] + ' SPEAKER A: ' + input [2]\n",
    "    label = labels[i].split('+')\n",
    "\n",
    "    # NO_CD\n",
    "    p = make_prompt(input)\n",
    "    inputs = tokenizer(p, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    response = response[len(p):]\n",
    "    responses.append(response)\n",
    "\n",
    "    #CD\n",
    "    p_k = make_prompt(input, len(label))\n",
    "    inputs = tokenizer(p_k, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "    response_k = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    response_k = response_k[len(p):]\n",
    "\n",
    "    responses_k.append(response_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE RESULTS\n",
    "\n",
    "pd.DataFrame({'input': verbatims, 'hypothese': responses}).to_csv(output_path + '/beluga_labels_window3.csv', encoding = 'UTF-8', index=False) #NO-CD\n",
    "pd.DataFrame({'input': verbatims, 'hypothese': responses_k}).to_csv(output_path + '/beluga_labels__window3_k.csv', encoding = 'UTF-8', index=False) #CD"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
