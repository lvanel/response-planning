{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Training BART Model on Multi-Label Sequence Generation in English</h1>\n",
    "We load the data and and fine-tune a BART model on the task of <b>generating</b> a sequence of multiple labels associated with the next dialogue utterance.\n",
    "\n",
    "\n",
    "<i>vers. 10/2023</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Data Preprocessing & Model Initialisation </h3>\n",
    "\n",
    "First we load the appropriate dataset, process it into the proper format and initialise the model we want to fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "from tabulate import tabulate\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHOOSE MODEL SIZE\n",
    "model_size = \"large\" # 'base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET DATA\n",
    "\n",
    "if model_size == 'large':\n",
    "    model_name =  \"facebook/bart-large\"\n",
    "    output_path = \"/BART_Large\"\n",
    "\n",
    "else:\n",
    "    model_name =  \"facebook/bart-base\"\n",
    "    output_path = \"/BART_Base\"\n",
    "\n",
    "data = load_dataset(\"csv\", data_files={\"train\":\"/data/daily_dialog_train_next.csv\", \"validation\":\"/data/daily_dialog_val_next.csv\", \"test\": \"/data/en/daily_dialog_test_next.csv\"})\n",
    "train_data_txt, validation_data_txt = data['train'], data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    os.makedirs(output_path + '/results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD MODEL AND TOKENIZER\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#ACTIVATE CUDA\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "#check the device\n",
    "device = torch.device(0)\n",
    "model = model.to(device)\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZER PARAMETERS\n",
    "\n",
    "encoder_max_length = 256  \n",
    "decoder_max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET LABELS, AND LENGTH OF LABEL SEQUENCE\n",
    "label_vals = [x.split('+') for x in test_data_txt['label']]\n",
    "li_val = [len(tokenizer(x).input_ids) for x in test_data_txt['label']]\n",
    "lens_li_labels=[len(x) for x in label_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PEEK AT THE DATA\n",
    "example = data['train'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESS BATCH OF DATA\n",
    "\n",
    "def batch_tokenize_preprocess(batch, tokenizer, max_source_length, max_target_length):\n",
    "    source, target = batch[\"text\"], batch[\"label\"]\n",
    "    source_tokenized = tokenizer(\n",
    "        source, padding=\"max_length\", truncation=True, max_length=max_source_length\n",
    "    )\n",
    "    target_tokenized = tokenizer(\n",
    "        target, padding=\"max_length\", truncation=True, max_length=max_target_length\n",
    "    )\n",
    "\n",
    "    batch = {k: v for k, v in source_tokenized.items()}\n",
    "    # Ignore padding in the loss\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in l]\n",
    "        for l in target_tokenized[\"input_ids\"]\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESS THE DATASETS\n",
    "\n",
    "train_data = train_data_txt.map(\n",
    "    lambda batch: batch_tokenize_preprocess(\n",
    "        batch, tokenizer, encoder_max_length, decoder_max_length\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=train_data_txt.column_names,\n",
    ")\n",
    "\n",
    "validation_data = validation_data_txt.map(\n",
    "    lambda batch: batch_tokenize_preprocess(\n",
    "        batch, tokenizer, encoder_max_length, decoder_max_length\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=validation_data_txt.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESS THE TEXT\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training Arguments and Metrics</h3>\n",
    "We then set the training hyper-parameters such as batch-size or number of epochs, and then we define the metrics we will compute during training and evaluation of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET THE ARGUMENTS AND HYPER-PARAMETERS\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "metric_name = \"f1\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir= output_path + \"/results\",\n",
    "    num_train_epochs=10,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=3e-05,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,\n",
    "    gradient_accumulation_steps = 2,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir= output_path + \"/logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=metric_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START WANDB SESSION TO KEEP TRACK OF TRAINING \n",
    "\n",
    "import wandb\n",
    "wandb_run = wandb.init(\n",
    "    project=\"bart\",\n",
    "    config={\n",
    "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"dataset\": dataset_name,\n",
    "    },\n",
    ")\n",
    "\n",
    "wandb_run.name = \"run_\" + \"bart\"  +  \"_\" model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COLLATE DATA\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MULTILABEL METRICS\n",
    "# while training, we need to define a `compute_metrics` function, that returns a dictionary with the desired metric values\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    f1_micro_average = f1_score(y_true=decoded_labels, y_pred=decoded_preds, average='micro')\n",
    "    roc_auc = roc_auc_score(decoded_labels, decoded_preds, average = 'micro')\n",
    "    accuracy = accuracy_score(decoded_labels, decoded_preds)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "            'roc_auc': roc_auc,\n",
    "            'accuracy': accuracy}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training</h3>\n",
    "Time to train and evaluate the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrainer = Seq2SeqTrainer(\\n    model=model,\\n    args=training_args,\\n    data_collator=data_collator,\\n    train_dataset=train_data,\\n    eval_dataset=validation_data,\\n    tokenizer=tokenizer,\\n    compute_metrics=compute_metrics,\\n)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialise trainer module\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=validation_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN!\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATION ON VALIDATION SET\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prediction and Inference</h3>\n",
    "\n",
    "Now that our model has been fine-tuned, we can use it to predict labels on brand new data it has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Hey man , you wanna buy some weed ? ', 'label': 'question+surprise'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PEEK INTO TEST DATA\n",
    "\n",
    "test = data['test'][0]\n",
    "test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#LOAD THE MODEL\n",
    "#Setting `problem_type` to be \"multi_label_classification\" makes sure we use the appropriate loss function, BCEWithLogitsLoss\n",
    "#The output layer has `len(labels)` output neurons, and we set the id2label and label2id mappings.\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "LOAD_MODEL = False\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    model_name_or_path = '/path/to/model'\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "else:\n",
    "    model = trainer.model\n",
    "\n",
    "\n",
    "device = torch.device(0)\n",
    "model = model.to(device)\n",
    "t = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICT/INFERENCE\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_summary_k(test_samples, li, model, k_generation=False):\n",
    "    samples = test_samples[\"text\"]\n",
    "    generated_output_str = []\n",
    "    lens = []\n",
    "\n",
    "    for i in tqdm(range(len(samples))):\n",
    "        inputs = tokenizer(\n",
    "            samples[i],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=encoder_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = inputs.input_ids.to(model.device)\n",
    "        attention_mask = inputs.attention_mask.to(model.device)\n",
    "        if k_generation:\n",
    "            outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=li[i]+2, min_new_tokens=li[i]+2)\n",
    "        else:\n",
    "            outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "        output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        output_str = output_str[0].replace('+', ', ')\n",
    "        generated_output_str.append(output_str)\n",
    "        lens.append(len(outputs))\n",
    "    return generated_output_str, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = test_data_txt\n",
    "labels_after_tuning, lens_labels = generate_summary_k(test_samples, li_val, model, k_generation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_after_tuning[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lens_li_labels[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE RESULTS\n",
    "test_df = pd.DataFrame(\n",
    "    {'segment': test_samples[\"text\"], \n",
    "    'reference':test_samples[\"label\"], \n",
    "    'expected li':lens_li_labels, \n",
    "    'length':lens_labels, \n",
    "    'hypothese': labels_after_tuning}\n",
    "    )\n",
    "\n",
    "test_df.to_csv(output_path +'/results/results.csv', index = False, encoding = 'UTF-8')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
