{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Process & Compare Predicted Results to Expected Sequences </h1>\n",
    "\n",
    "<i>vers. 10/23</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all data\n",
    "\n",
    "#BERT\n",
    "path_bert = 'path/to/results'\n",
    "data_bert = pd.read_csv(path_bert, encoding = 'UTF-8')\n",
    "\n",
    "\n",
    "# BART \n",
    "path_bart= 'path/to/results'\n",
    "path_bart_cd = 'path/to/results'\n",
    "\n",
    "data_bart = pd.read_csv(path_bart, encoding = 'UTF-8')\n",
    "data_bart_cd = pd.read_csv(path_bart_cd, encoding = 'UTF-8')\n",
    "\n",
    "\n",
    "#BELUGA\n",
    "path_beluga= 'path/to/results'\n",
    "path_beluga_cd = 'path/to/results'\n",
    "\n",
    "data_beluga = pd.read_csv(path_beluga, encoding = 'UTF-8')\n",
    "data_beluga_cd = pd.read_csv(path_beluga_cd, encoding = 'UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET REFERENCE LABELS\n",
    "\n",
    "ref = pd.read_csv('data/daily_dialog_test_next_window3.csv')\n",
    "print(len(ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LABELS\n",
    "\n",
    "acts = ['inform', 'question', 'directive', 'commissive']\n",
    "emos= ['neutral', 'anger', 'disgust', 'fear', 'happiness', 'sadness','surprise']\n",
    "labels = acts + emos\n",
    "\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET LABELS FROM REFERENCE + Li (LENGTH OF SEQUENCES)\n",
    "\n",
    "labels_data = [x.split('+') for x in ref['label']]\n",
    "li = [len(x) for x in labels_data]\n",
    "\n",
    "len(labels_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>BART</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_true_pred(pred, labels):\n",
    "    match = []\n",
    "    for x in pred:\n",
    "        flag = False\n",
    "        idx = 0\n",
    "        while idx < len(labels) and not flag:\n",
    "            label = labels[idx]\n",
    "            if x in label or label in x or x == label:\n",
    "                match.append(label)\n",
    "                flag = True\n",
    "            \n",
    "            idx += 1\n",
    "    \n",
    "    if not flag:\n",
    "        print('NO')\n",
    "        \n",
    "    return match\n",
    "\n",
    "\n",
    "def get_labels(data, labels):\n",
    "    preds = data['hypothese']\n",
    "    preds = [x.split('+') if '+' in x else x.split(', ') for x in preds]\n",
    "\n",
    "    clean_preds = []\n",
    "    c =0\n",
    "\n",
    "    for pred in preds:\n",
    "        x = find_true_pred(pred, labels)\n",
    "\n",
    "\n",
    "        if len(x) <1:\n",
    "            #print(pred)\n",
    "            c += 1\n",
    "        clean_preds.append(x)\n",
    "\n",
    "    \n",
    "    print(\"NUMBR OF EMPTY PREDS : \", c)\n",
    "                \n",
    "    return clean_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET PREDS BART NO-CD\n",
    "\n",
    "preds_bart = get_labels(data_bart, labels)\n",
    "\n",
    "preds_bart[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET PREDS BART CD\n",
    "\n",
    "preds_bart_cd = get_labels(data_bart_cd, labels)\n",
    "\n",
    "preds_bart_cd[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(preds_bart), len(preds_bart_cd))\n",
    "print(len(labels_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>BELUGA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_labels(hypothese, labels):\n",
    "    flag = False\n",
    "    idxs = []\n",
    "    ls = []\n",
    "    for label in labels:\n",
    "        if label in hypothese:\n",
    "            flag = True\n",
    "            print(hypothese)\n",
    "            label_idx = [m.start() for m in re.finditer(label, hypothese)]\n",
    "            for j in label_idx:\n",
    "                idxs.append(j)\n",
    "                ls.append(label)\n",
    "\n",
    "    if flag:    \n",
    "        sorted_idxs = idxs\n",
    "        sorted_idxs.sort()\n",
    "        sorted_labels = [ls[idxs.index(i)] for i in idxs]\n",
    "    \n",
    "    else:\n",
    "        #print('wtf, ', hypothese)\n",
    "        sorted_labels = ['Empty']\n",
    "\n",
    "    return sorted_labels\n",
    "\n",
    "\n",
    "def get_labels_beluga(df, labels):\n",
    "    empty = 0\n",
    "    responses = []\n",
    "\n",
    "    for i, row in tqdm(df.iterrows()):\n",
    "        hypothese = row['hypothese']\n",
    "        if isinstance(hypothese, str):\n",
    "            hypothese = hypothese.strip()\n",
    "            if len(hypothese) > 3:\n",
    "                response= get_labels(hypothese, labels)\n",
    "                responses.append(response)\n",
    "                if response == ['Empty']: \n",
    "                    empty += 1\n",
    "\n",
    "            else:\n",
    "                responses.append(['Empty'])\n",
    "                empty += 1\n",
    "\n",
    "        else:\n",
    "            empty += 1\n",
    "            responses.append(['Empty'])\n",
    "\n",
    "\n",
    "    print('EMPTY PREDS : ', empty, ' OUT OF ', len(df))\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET BELUGA NO-CD PREDS\n",
    "\n",
    "preds_beluga = get_labels_beluga(data_beluga, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_beluga[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET BELUGA CD PREDS\n",
    "\n",
    "preds_beluga_cd = get_labels_beluga(data_beluga_cd, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>BERT</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(df):\n",
    "    scores = []\n",
    "    labels = []\n",
    "    columns = [x for x in df.columns if x not in ['input','reference']]\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        score = []\n",
    "        for column in columns:\n",
    "            score.append(float(row[column]))\n",
    "        \n",
    "        sorted_score = sorted(score, reverse=True)\n",
    "        label = []\n",
    "        for s in sorted_score:\n",
    "            idx = score.index(s)\n",
    "            label.append(columns[idx])\n",
    "        labels.append(label)\n",
    "        scores.append([round(x, 3) for x in sorted_score])\n",
    "\n",
    "\n",
    "    return labels, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresh_labels(df, threshold = 0.3, k = None, k_list = None):\n",
    "    preds, scores = get_scores(df)\n",
    "    new_preds = []\n",
    "    new_scores = []\n",
    "\n",
    "    if k_list is not None:\n",
    "        for i in range(len(preds)):\n",
    "            new_preds.append(preds[i][:k_list[i]])\n",
    "            new_scores.append(scores[i][:k_list[i]])\n",
    "\n",
    "    elif k is not None:\n",
    "        for i in range(len(preds)):\n",
    "            new_preds.append(preds[i][:k])\n",
    "            new_scores.append(scores[i][:k])\n",
    "    \n",
    "    else:\n",
    "\n",
    "        for i in range(len(preds)):\n",
    "            scores_i = scores[i]\n",
    "            idx = 0\n",
    "            flag = True\n",
    "            while idx < len(scores_i) and flag:\n",
    "                if scores_i[idx] < threshold:\n",
    "                    flag = False\n",
    "                else:\n",
    "                    idx+= 1\n",
    "            new_preds.append(preds[i][:idx])\n",
    "            new_scores.append(scores_i[:idx])\n",
    "    \n",
    "    return new_preds, new_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET BERT NO-CD PREDS\n",
    "#THRESH = 0.7 FOR BART BASE, 0.5 FOR BART LARGE\n",
    "\n",
    "preds_bert, scores_bert = thresh_labels(data_bert, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_bert[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET BERT CD PREDS\n",
    "#take top li predictions\n",
    "\n",
    "preds_bert_cd, scores_bert_cd = thresh_labels(data_bert, k_list = li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_bert_cd[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Classifier</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at distribution of labels sequences length in dataset\n",
    "\n",
    "dic = {'ones': len([x for x in labels_data if len(x)==1]), 'twos':len([x for x in labels_data if len(x)==2]), 'all': len(labels_data)}\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly select labels according to the distribution\n",
    "\n",
    "from random import randint, choice\n",
    "import numpy as np\n",
    "\n",
    "def random_classifier(data, labels, k = 2, k_list = None):\n",
    "\n",
    "    labels_random = []        \n",
    "    for i in range(len(data)):\n",
    "        k = np.random.choice(np.arange(1, 3), p=[0.8, 0.2])\n",
    "\n",
    "        turn_labels = []\n",
    "\n",
    "        #if k_list is not None:\n",
    "        #    k = k_list[i]\n",
    "\n",
    "        for j in range(k):\n",
    "            rand_idx = randint(0, len(labels)-1)\n",
    "            turn_labels.append(labels[rand_idx])\n",
    "\n",
    "        labels_random.append(turn_labels)\n",
    "\n",
    "    return labels_random\n",
    "\n",
    "\n",
    "preds_random = random_classifier(labels_data, labels, k = 2)\n",
    "preds_random_li = random_classifier(labels_data, labels, k_list = li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Compute Scores</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Comparison Score</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def mean_turn_li(preds):\n",
    "    lens_labels = []\n",
    "    c = 0\n",
    "    nb = 0\n",
    "    max_l = 0\n",
    "    min_l = 10000\n",
    "\n",
    "    for i in range(len(preds)):\n",
    "        nb +=1\n",
    "        pred = preds[i]\n",
    "        lens_labels.append(len(pred))\n",
    "        if len(pred) ==0:\n",
    "            c+= 1\n",
    "\n",
    "        if len(pred) > max_l:\n",
    "            max_l = len(pred)\n",
    "            max_i = i\n",
    "\n",
    "        if len(pred) < min_l:\n",
    "            min_l = len(pred)\n",
    "            min_i = i\n",
    "        \n",
    "    print(\"AVERAGE NUMBER OF LABELS PER SPEAKER TURN\", sum(lens_labels)/nb)\n",
    "    print(\"MAX NUMBER OF LABELS PER SPEAKER TURN\", max_l)\n",
    "    print(\"MIN NUMBER OF LABELS PER SPEAKER TURN\", min_l)\n",
    "    print(\"MEDIAN NUMBER OF LABELS PER SPEAKER TURN\", statistics.median(lens_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETS STATS\n",
    "\n",
    "print('REFERENCE')\n",
    "mean_turn_li(labels_data)\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('BERT')\n",
    "mean_turn_li(preds_bert)\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('BERT LI')\n",
    "mean_turn_li(preds_bert_cd)\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('BART')\n",
    "mean_turn_li(preds_bart)\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('BART LI')\n",
    "mean_turn_li(preds_bart_cd)\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('RANDOM')\n",
    "mean_turn_li(preds_random)\n",
    "\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('RANDOM LI')\n",
    "mean_turn_li(preds_random_li)\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('BELUGA')\n",
    "mean_turn_li(preds_beluga)\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "\n",
    "print('BELUGA LI')\n",
    "mean_turn_li(preds_beluga_cd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Multi-Label F1/Accuracy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPUTE ONE HOT ENCODE\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def one_hot_encode(preds, labels):\n",
    "    ohe = np.zeros((len(preds), len(labels)))\n",
    "    for k in range(len(preds)):\n",
    "        for i in range(len(preds[k])):\n",
    "            for j in range(len(labels)):\n",
    "                if preds[k][i] == labels[j]:\n",
    "                    ohe[k][j] = 1\n",
    "                \n",
    "                else:\n",
    "                    ohe[k][j] = 0\n",
    "    return ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Empty for unparsed Beluga predictions\n",
    "\n",
    "labels += ['Empty']\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT PREDS TO OHE\n",
    "\n",
    "labels_bert_ohe = one_hot_encode(preds_bert, labels)\n",
    "labels_beluga_ohe = one_hot_encode(preds_beluga, labels)\n",
    "labels_random_ohe = one_hot_encode(preds_random, labels)\n",
    "labels_bart_ohe = one_hot_encode(preds_bart, labels)\n",
    "\n",
    "\n",
    "labels_data_ohe = one_hot_encode(labels_data, labels)\n",
    "\n",
    "\n",
    "labels_bert_li_ohe = one_hot_encode(preds_bert_cd, labels)\n",
    "labels_beluga_li_ohe = one_hot_encode(preds_beluga_cd, labels)\n",
    "labels_random_li_ohe = one_hot_encode(preds_random_li, labels)\n",
    "labels_bart_li_ohe = one_hot_encode(preds_bart_cd, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, classification_report, jaccard_score\n",
    "from textdistance import levenshtein\n",
    "\n",
    "\n",
    "print('Jaccard score of beluga : ', jaccard_score(labels_beluga_ohe, labels_data_ohe, average = 'weighted'))\n",
    "\n",
    "print(classification_report(labels_beluga_ohe, labels_data_ohe, target_names= labels))\n",
    "\n",
    "\n",
    "levsim = levenshtein.normalized_similarity(preds_beluga,labels_data)\n",
    "\n",
    "print('LEVENSHTEIN SIM beluga : ', levsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Jaccard score of beluga li: ', jaccard_score(labels_beluga_li_ohe, labels_data_ohe, average = 'weighted'))\n",
    "\n",
    "print(classification_report(labels_beluga_li_ohe, labels_data_ohe, target_names= labels))\n",
    "\n",
    "\n",
    "levsim = levenshtein.normalized_similarity(preds_beluga_cd,labels_data)\n",
    "\n",
    "print('LEVENSHTEIN SIM beluga li: ', levsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, classification_report, jaccard_score\n",
    "from textdistance import levenshtein\n",
    "\n",
    "\n",
    "print('Jaccard score of multilabel bert : ', jaccard_score(labels_bert_ohe, labels_data_ohe, average = 'weighted'))\n",
    "\n",
    "print(classification_report(labels_bert_ohe, labels_data_ohe, target_names= labels))\n",
    "\n",
    "\n",
    "levsim = levenshtein.normalized_similarity(preds_bert,labels_data)\n",
    "\n",
    "print('LEVENSHTEIN SIM BERT : ', levsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Jaccard score of multilabel bert li: ', jaccard_score(labels_bert_li_ohe, labels_data_ohe, average = 'weighted'))\n",
    "\n",
    "print(classification_report(labels_bert_li_ohe, labels_data_ohe, target_names= labels))\n",
    "\n",
    "\n",
    "levsim = levenshtein.normalized_similarity(preds_bert_cd,labels_data)\n",
    "\n",
    "print('LEVENSHTEIN SIM BERT LI: ', levsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Jaccard score of bart ohe: ', jaccard_score(labels_bart_ohe, labels_data_ohe, average = 'weighted'))\n",
    "\n",
    "print(classification_report(labels_bart_ohe, labels_data_ohe, target_names= labels))\n",
    "\n",
    "levsim = levenshtein.normalized_similarity(preds_bart,labels_data)\n",
    "\n",
    "print('LEVENSHTEIN SIM BART : ', levsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Jaccard score of bart  ohe li: ', jaccard_score(labels_bart_li_ohe, labels_data_ohe, average = 'weighted'))\n",
    "\n",
    "print(classification_report(labels_bart_li_ohe, labels_data_ohe, target_names= labels))\n",
    "\n",
    "levsim = levenshtein.normalized_similarity(preds_bart_cd,labels_data)\n",
    "\n",
    "print('LEVENSHTEIN SIM BART LI: ', levsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Jaccard score of random : ', jaccard_score(labels_random_ohe, labels_data_ohe, average = 'weighted'))\n",
    "\n",
    "print(classification_report(labels_random_ohe, labels_data_ohe, target_names= labels))\n",
    "\n",
    "levsim = levenshtein.normalized_similarity(preds_random,labels_data)\n",
    "\n",
    "print('LEVENSHTEIN SIM RANDOM : ', levsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Jaccard score of random li: ', jaccard_score(labels_random_li_ohe, labels_data_ohe, average = 'weighted'))\n",
    "\n",
    "print(classification_report(labels_random_li_ohe, labels_data_ohe, target_names= labels))\n",
    "\n",
    "levsim = levenshtein.normalized_similarity(preds_random_li,labels_data)\n",
    "\n",
    "print('LEVENSHTEIN SIM RANDOM LI: ', levsim)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
