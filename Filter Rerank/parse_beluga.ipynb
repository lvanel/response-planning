{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Parse Beluga Results</h1>\n",
    "Extract from the Beluga outputs the actual responses.\n",
    "\n",
    "\n",
    "<i>vers. 10/2023</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os \n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "device = torch.device(0)\n",
    "\n",
    "#solo refers to BELUGA NO-CD (one response is generated), multi refers to BELUGA CD1/2 when it generated N=10 candidates\n",
    "\n",
    "beluga_solo = 'Response Generation/beluga/beluga_labels.csv'\n",
    "beluga_multi = 'Response Generation/beluga/beluga_labels_k_10.csv'\n",
    "\n",
    "df_solo = pd.read_csv(beluga_solo, encoding = 'UTF-8')\n",
    "df_multi = pd.read_csv(beluga_multi, encoding = 'UTF-8')\n",
    "\n",
    "#References\n",
    "references= 'Response Generation/bart/bart_epochs_10_generated_responses_daily_dialog_window3_new.csv'\n",
    "refs = pd.read_csv(references, encoding = 'UTF-8')['actual_responses']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Solo Results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>hypothese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Hey man , you wanna buy some weed ? ', ' Som...</td>\n",
       "      <td>SPEAKER B:  No thanks, man , I don't do that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[' Weed ! You know ? Pot , Ganja , Mary Jane s...</td>\n",
       "      <td>\\n\\n Response: I appreciate your offer, but I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[' I also have blow if you prefer to do a few ...</td>\n",
       "      <td>SPEAKER B:  I am good thank you , I don't do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[' Come on man ! I even got dope and acid ! Tr...</td>\n",
       "      <td>\\nResponse: I'm sorry , but I don't do drugs.\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[' I got my connections ! Just tell me what yo...</td>\n",
       "      <td>\\n\\nResponse: Just a minute. While we're waiti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  ['Hey man , you wanna buy some weed ? ', ' Som...   \n",
       "1  [' Weed ! You know ? Pot , Ganja , Mary Jane s...   \n",
       "2  [' I also have blow if you prefer to do a few ...   \n",
       "3  [' Come on man ! I even got dope and acid ! Tr...   \n",
       "4  [' I got my connections ! Just tell me what yo...   \n",
       "\n",
       "                                           hypothese  \n",
       "0   SPEAKER B:  No thanks, man , I don't do that ...  \n",
       "1  \\n\\n Response: I appreciate your offer, but I ...  \n",
       "2   SPEAKER B:  I am good thank you , I don't do ...  \n",
       "3  \\nResponse: I'm sorry , but I don't do drugs.\\...  \n",
       "4  \\n\\nResponse: Just a minute. While we're waiti...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_solo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = 0\n",
    "indicators = ['Response: ', 'response: ', 'SPEAKER A: ', 'SPEAKER B: ', 'A: ', 'B: ']\n",
    "responses = []\n",
    "\n",
    "for i, row in tqdm(df_solo.iterrows()):\n",
    "    hypothese = row['hypothese']\n",
    "    if isinstance(hypothese, str):\n",
    "        hypothese = hypothese.strip()\n",
    "        flag = False\n",
    "        idxs = []\n",
    "        inds = []\n",
    "        for x in indicators:\n",
    "            if x in hypothese:\n",
    "                flag = True\n",
    "                indicator_idx = [m.start() for m in re.finditer(x, hypothese)]\n",
    "                print(indicator_idx)\n",
    "                for j in indicator_idx:\n",
    "                    idxs.append(j)\n",
    "                    inds.append(x)\n",
    "\n",
    "        if flag:    \n",
    "            max_idx = idxs.index(max(idxs))\n",
    "            max_indicator = inds[max_idx]\n",
    "\n",
    "            response = hypothese[idxs[max_idx] + len(max_indicator):]\n",
    "            responses.append(response.strip())\n",
    "\n",
    "        else:\n",
    "            if len(hypothese) > 3:\n",
    "                responses.append(hypothese)\n",
    "            \n",
    "            else:\n",
    "                responses.append('None')\n",
    "                empty += 1\n",
    "\n",
    "    else:\n",
    "        empty += 1\n",
    "        responses.append('None')\n",
    "\n",
    "\n",
    "print('EMPTY PREDS : ', empty, ' OUT OF ', len(df_solo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'Response Generation/beluga/'\n",
    "\n",
    "file_generated = output_path + \"beluga_generated_responses_daily_dialog_window_3_N1\"\n",
    "\n",
    "df = pd.DataFrame({'input': df_solo['input'], 'hypothese': responses})\n",
    "#df.to_csv(file_generated+'.csv', index=False, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_responses = responses\n",
    "actual_responses = refs\n",
    "\n",
    "assert(len(generated_responses)==len(actual_responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT METRICS\n",
    "\n",
    "import evaluate\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "chrf = evaluate.load(\"chrf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEEK AT THE RESULTS\n",
    "actual_responses = [[res] for res in actual_responses] #Refs must be in a list of list of str\n",
    "\n",
    "print(df_solo['input'][:5])\n",
    "print(generated_responses[:5])\n",
    "print(actual_responses[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPUTE METRICS\n",
    "\n",
    "bleu_score = sacrebleu.compute(predictions=generated_responses, references=actual_responses)\n",
    "\n",
    "rouge_score = rouge.compute(predictions=generated_responses, references=actual_responses)\n",
    "\n",
    "bert_score = bertscore.compute(predictions=generated_responses, references=actual_responses, lang='en')\n",
    "precision = bert_score['precision']\n",
    "recall = bert_score['recall']\n",
    "f1 = bert_score['f1']\n",
    "avg_precision_bert = sum(precision) / len(precision)\n",
    "avg_recall_bert = sum(recall) / len(recall)\n",
    "avg_f1_bert = sum(f1) / len(f1)\n",
    "\n",
    "chrf_score = chrf.compute(predictions=generated_responses, references=actual_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITE METRICS FOR SOLO\n",
    "import sys\n",
    "\n",
    "fout = open(file_generated+\".txt\", \"w\")\n",
    "print('Bleu score: \\n', bleu_score) #Range from 0 to 100\n",
    "print('Rouge score: \\n', rouge_score)\n",
    "print('Bert score: \\n', bert_score)\n",
    "print('Avg precision Bert score: ', avg_precision_bert)\n",
    "print('Avg recall Bert score: ', avg_recall_bert)\n",
    "print('Avg f1 Bert score: ', avg_f1_bert)\n",
    "print('chrf score: \\n', chrf_score)\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>MULTI Results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>hypothese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Hey man , you wanna buy some weed ? ', ' Som...</td>\n",
       "      <td>Hey man, you wanna buy some weed? 2: No, I'm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[' Weed ! You know ? Pot , Ganja , Mary Jane s...</td>\n",
       "      <td>Weed ! You know ? Pot , Ganja , Mary Jane som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[' I also have blow if you prefer to do a few ...</td>\n",
       "      <td>2: 3: 4: 5: 6: 7: 8: 9: 10:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[' Come on man ! I even got dope and acid ! Tr...</td>\n",
       "      <td>1. SPEAKER A: Come on man! I even got dope and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[' I got my connections ! Just tell me what yo...</td>\n",
       "      <td>I want some juicy gossip! 2:  I want the late...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  ['Hey man , you wanna buy some weed ? ', ' Som...   \n",
       "1  [' Weed ! You know ? Pot , Ganja , Mary Jane s...   \n",
       "2  [' I also have blow if you prefer to do a few ...   \n",
       "3  [' Come on man ! I even got dope and acid ! Tr...   \n",
       "4  [' I got my connections ! Just tell me what yo...   \n",
       "\n",
       "                                           hypothese  \n",
       "0   Hey man, you wanna buy some weed? 2: No, I'm ...  \n",
       "1   Weed ! You know ? Pot , Ganja , Mary Jane som...  \n",
       "2                        2: 3: 4: 5: 6: 7: 8: 9: 10:  \n",
       "3  1. SPEAKER A: Come on man! I even got dope and...  \n",
       "4   I want some juicy gossip! 2:  I want the late...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_multi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentences(hypothese, indexes, indicators):\n",
    "    clean_responses=[]\n",
    "    count = len([indexes[x] for x in range(len(indexes)) if indexes[x] >=0])\n",
    "\n",
    "    if count <2:\n",
    "        print('wtf')\n",
    "\n",
    "    if indexes[0] == -1 and indexes[1] != -1:\n",
    "        res= hypothese[:indexes[1]].strip()\n",
    "        if len(res) > 2:\n",
    "            clean_responses.append(res)\n",
    "            count += 1\n",
    "\n",
    "    i = 0\n",
    "    while i <len(indexes):\n",
    "        if indicators[i] != -1:\n",
    "            j = i + 1\n",
    "            flag = False\n",
    "            while j < len(indexes) and not flag:\n",
    "                if indexes[j] != -1:\n",
    "                    res = hypothese[indexes[i]+len(indicators[i]): indexes[j]].strip()\n",
    "                    if len(res) >2:\n",
    "                        clean_responses.append(res)\n",
    "                    flag = True\n",
    "                    \n",
    "                j+= 1\n",
    "\n",
    "        i+=1\n",
    "    \n",
    "    if indexes[len(indexes)-1] != -1:\n",
    "        res= hypothese[indexes[len(indexes)-1]+len(indicators[len(indexes)-1]):].strip()\n",
    "        if len(res) > 2:\n",
    "            clean_responses.append(res)\n",
    "    \n",
    "    if len(clean_responses) != 10:\n",
    "        add_n = 10 - len(clean_responses)\n",
    "        clean_responses += ['None']*add_n\n",
    "\n",
    "    assert(len(clean_responses)==10)\n",
    "    #print(count, len(clean_responses))\n",
    "    return clean_responses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "empty = 0\n",
    "#indicators = [str(k) + ':' for k in range(11)] + [str(k) + ')' for k in range(11)] + [str(k) + '.' for k in range(11)]\n",
    "#indicators = ['1:', '1)', '1.', '1'] + ['2:', '2)', '2.', '2']\n",
    "indicators = [':', ')', '.', ' ']\n",
    "\n",
    "\n",
    "responses = []\n",
    "\n",
    "for i, row in tqdm(df_multi.iterrows()):\n",
    "    hypothese = row['hypothese']\n",
    "    if isinstance(hypothese, str):\n",
    "        hypothese = hypothese.strip()\n",
    "        idxs = []\n",
    "        indic = []\n",
    "        for x in range(1, 11):\n",
    "            flag = False\n",
    "            j = 0\n",
    "            while j <len(indicators) and not flag:\n",
    "                indicator = indicators[j]\n",
    "                if str(x) +indicator in hypothese:\n",
    "                    new_idx = hypothese.index(str(x) +indicator)\n",
    "                    idxs.append(new_idx)\n",
    "                    indic.append(str(x) +indicator)\n",
    "                    flag = True\n",
    "                \n",
    "                j+= 1\n",
    "            \n",
    "            if not flag:\n",
    "                idxs.append(-1)\n",
    "                indic.append(-1)\n",
    "\n",
    "        if idxs == [-1]*10:\n",
    "            responses.append(['None']*10)\n",
    "            empty += 1\n",
    "        \n",
    "        else:\n",
    "            response = find_sentences(hypothese, idxs, indic)\n",
    "            responses.append(response)\n",
    "\n",
    "\n",
    "    else:\n",
    "        empty += 1\n",
    "\n",
    "print('EMPTY PREDS : ', empty, ' OUT OF ', len(df_solo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "lens = [len(x) for x in responses]\n",
    "mean(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_null = []\n",
    "for res in responses:\n",
    "    nn =  [r for r in res if r != 'None']\n",
    "    not_null.append(nn)\n",
    "\n",
    "lens_not_null = [len(x) for x in not_null]\n",
    "mean(lens_not_null)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_generated = 'Response Generation/responses/beluga_generated_multiple_responses_daily_dialog_window3_N10'\n",
    "print(file_generated)\n",
    "N=10\n",
    "\n",
    "df = pd.DataFrame({'inputs': df_multi['input'],'actual responses':actual_responses})\n",
    "\n",
    "for res in range(N):\n",
    "    df['generated_responses_'+ str(res)] = [x[res] for x in responses]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(file_generated +'.csv', index = False, encoding = 'UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Prompt Based Approach: Expected vs Predicted Labels </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "beluga_expected = 'Response Generation/beluga/beluga_labels_expected.csv'\n",
    "beluga_predicted = 'Response Generation/beluga/beluga_labels_pred.csv'\n",
    "\n",
    "references= 'Response Generation/bart/bart_epochs_10_generated_responses_daily_dialog_window3_new.csv'\n",
    "\n",
    "df_expected = pd.read_csv(beluga_expected, encoding = 'UTF-8')\n",
    "df_predicted = pd.read_csv(beluga_predicted, encoding = 'UTF-8')\n",
    "refs = pd.read_csv(references, encoding = 'UTF-8')['actual_responses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = 0\n",
    "#indicators = ['Response: ', 'response: ', 'SPEAKER A: ', 'SPEAKER B: ', 'A: ', 'B: ']\n",
    "responses = []\n",
    "\n",
    "mode = 'predicted' #'expected' #\n",
    "\n",
    "if mode == 'predicted':\n",
    "    df = df_predicted\n",
    "\n",
    "else:\n",
    "    df = df_expected\n",
    "\n",
    "\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    hypothese = row['hypothese']\n",
    "    if isinstance(hypothese, str):\n",
    "        hypothese = hypothese.strip()\n",
    "        flag = False\n",
    "\n",
    "        if len(hypothese) > 2:\n",
    "            responses.append(hypothese)\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(i, hypothese.strip())\n",
    "            empty += 1\n",
    "            responses.append('None')\n",
    "\n",
    "    else:\n",
    "        empty += 1\n",
    "        print(i, hypothese)\n",
    "\n",
    "        responses.append('None')\n",
    "\n",
    "\n",
    "print('EMPTY PREDS : ', empty, ' OUT OF ', len(df_solo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_responses = responses\n",
    "actual_responses = refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>hypothese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Hey man , you wanna buy some weed ? ', ' Som...</td>\n",
       "      <td>What are you talking about? I don't understand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[' Weed ! You know ? Pot , Ganja , Mary Jane s...</td>\n",
       "      <td>Uh, no, I'm good on that front. Thanks though.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[' I also have blow if you prefer to do a few ...</td>\n",
       "      <td>No thanks, I am good with what I have.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[' Come on man ! I even got dope and acid ! Tr...</td>\n",
       "      <td>Can you give me some information about where y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[' I got my connections ! Just tell me what yo...</td>\n",
       "      <td>Alright, what are you after? Let me know what ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>['The taxi drivers are on strike again . ', ' ...</td>\n",
       "      <td>What is the reason behind the taxi drivers' st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[\"We've managed to reduce our energy consumpti...</td>\n",
       "      <td>How have you invested in a heat recovery syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[\" Mainly because we've invested in a heat rec...</td>\n",
       "      <td>Investing in a heat recovery system has allowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>['Believe it or not , tea is the most popular ...</td>\n",
       "      <td>Do you know which type of tea is most popular ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[' Right . And China is the homeland of tea . ...</td>\n",
       "      <td>I have actually never tasted any Chinese tea, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  ['Hey man , you wanna buy some weed ? ', ' Som...   \n",
       "1  [' Weed ! You know ? Pot , Ganja , Mary Jane s...   \n",
       "2  [' I also have blow if you prefer to do a few ...   \n",
       "3  [' Come on man ! I even got dope and acid ! Tr...   \n",
       "4  [' I got my connections ! Just tell me what yo...   \n",
       "5  ['The taxi drivers are on strike again . ', ' ...   \n",
       "6  [\"We've managed to reduce our energy consumpti...   \n",
       "7  [\" Mainly because we've invested in a heat rec...   \n",
       "8  ['Believe it or not , tea is the most popular ...   \n",
       "9  [' Right . And China is the homeland of tea . ...   \n",
       "\n",
       "                                           hypothese  \n",
       "0  What are you talking about? I don't understand...  \n",
       "1     Uh, no, I'm good on that front. Thanks though.  \n",
       "2             No thanks, I am good with what I have.  \n",
       "3  Can you give me some information about where y...  \n",
       "4  Alright, what are you after? Let me know what ...  \n",
       "5  What is the reason behind the taxi drivers' st...  \n",
       "6  How have you invested in a heat recovery syste...  \n",
       "7  Investing in a heat recovery system has allowe...  \n",
       "8  Do you know which type of tea is most popular ...  \n",
       "9  I have actually never tasted any Chinese tea, ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_generated = \"Filter Rerank/en/results_filter/beluga_final_responses_\"+mode\n",
    "\n",
    "df = pd.DataFrame({'input': df_solo['input'], 'hypothese': responses})\n",
    "\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(file_generated+'.csv', index=False, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score = sacrebleu.compute(predictions=generated_responses, references=actual_responses)\n",
    "\n",
    "rouge_score = rouge.compute(predictions=generated_responses, references=actual_responses)\n",
    "\n",
    "bert_score = bertscore.compute(predictions=generated_responses, references=actual_responses, lang='en')\n",
    "precision = bert_score['precision']\n",
    "recall = bert_score['recall']\n",
    "f1 = bert_score['f1']\n",
    "avg_precision_bert = sum(precision) / len(precision)\n",
    "avg_recall_bert = sum(recall) / len(recall)\n",
    "avg_f1_bert = sum(f1) / len(f1)\n",
    "\n",
    "chrf_score = chrf.compute(predictions=generated_responses, references=actual_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "fout = open(file_generated+\".txt\", \"w\")\n",
    "print('Bleu score: \\n', bleu_score) #Range from 0 to 100\n",
    "print('Rouge score: \\n', rouge_score)\n",
    "print('Bert score: \\n', bert_score)\n",
    "print('Avg precision Bert score: ', avg_precision_bert)\n",
    "print('Avg recall Bert score: ', avg_recall_bert)\n",
    "print('Avg f1 Bert score: ', avg_f1_bert)\n",
    "print('chrf score: \\n', chrf_score)\n",
    "fout.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60449db133074e1cbee451fb5ae50da18c990312c4a52ac5f503573249ffb44a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.17 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
