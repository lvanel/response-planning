{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Filter & Rerank: Ranking & Selecting Candidate Responses using Multi-Label Sequences</h1>\n",
    "We look at the candidate responses generated previously. For each sample, for each candidate, we use BERT Current to extract the list of labels displayed in the candidate response. We compare this sequence to the \"expected sequence of labels\": observed in the dataset for CD1, generated by BART NO-CD for CD2. This allows the algorithm to select the top-scoring candidate.\n",
    "\n",
    "\n",
    "<i>vers. 10/2023</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT METRICS\n",
    "import evaluate, sys\n",
    "from statistics import mean\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "\n",
    "#COSINE SIMILARITY\n",
    "#!pip install sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "cosine_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUDA\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL\n",
    "\n",
    "#BERT CURRENT\n",
    "model_name_or_path = \"/path/to/BERT/Current\" #TODO\n",
    "model_name = \"bert-base\"\n",
    "\n",
    "prediction_model_path = 'path/to/BART/NO-CD' #TODO\n",
    "prediction_model_name = \"facebook/bart-base\"\n",
    "\n",
    "output_path = '/Filter Rerank//results_filter_window3'\n",
    "\n",
    "#RESPONSES CANDIDATES\n",
    "references_path_bart = \"Response Generation/responses/bart_generated_multiple_responses_daily_dialog_window3_N10.csv\"\n",
    "references_path_bart_large = \"Response Generation/responses/bart_LARGE_generated_multiple_responses_daily_dialog_window3_N10.csv\"\n",
    "\n",
    "references_path_dialoGPT = \"Response Generation/responses/dialogpt_generated_multiple_responses_daily_dialog_window3_N10.csv\"\n",
    "references_path_dialoGPT_large = \"Response Generation/responses/dialogpt_LARGE_generated_multiple_responses_daily_dialog_window3_N10.csv\"\n",
    "\n",
    "\n",
    "references_path_gpt2 = \"Response Generation/responses/gpt_generated_multiple_responses_daily_dialog_window3_N10.csv\"\n",
    "references_path_gpt2_large = \"Response Generation/responses/gpt2_LARGE_generated_multiple_responses_daily_dialog_window3_N10.csv\"\n",
    "\n",
    "references_path_beluga = \"Response Generation/responses/beluga_generated_multiple_responses_daily_dialog_window3_N10.csv\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIST OF CONFIGURATION: MODELS, CD1/CD2 FOR LOOPING THE PROCESS\n",
    "\n",
    "models_list= {'BART': references_path_bart, 'BART_Large': references_path_bart_large, 'dialoGPT': references_path_dialoGPT, 'dialoGPT_Large': references_path_dialoGPT_large, 'GPT2': references_path_gpt2, 'GPT2_Large': references_path_gpt2_large, 'Beluga': references_path_beluga}\n",
    "modes = ['CD1', 'CD2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET LABELS FROM DATASET FOR CD1 \n",
    "\n",
    "labels = pd.read_csv('Labels Prediction/data/daily_dialog_test_next_window3.csv', encoding = 'UTF-8')\n",
    "text = labels['text'].tolist()\n",
    "labels = labels['label'].tolist()\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> LOAD MODELS & GET REFERENCES FOR LABELS SEQUENCES</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD THE MODEL BERT CURRENT\n",
    "#Setting `problem_type` to be \"multi_label_classification\" makes sure we use the appropriate loss function, BCEWithLogitsLoss\n",
    "#The output layer has `len(labels)` output neurons, and we set the id2label and label2id mappings.\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "id2label = [config.id2label[key] for key in sorted(config.id2label.keys(), key=lambda t: int(t))]\n",
    "id2label = np.asarray(id2label)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\n",
    "device = torch.device(0)\n",
    "model = model.to(device)\n",
    "t = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD THE MODEL: BART NO-CD\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM\n",
    "import numpy as np\n",
    "\n",
    "encoder_max_length = 256  \n",
    "decoder_max_length = 64\n",
    "\n",
    "prediction_config = AutoConfig.from_pretrained(prediction_model_path)\n",
    "prediction_tokenizer = AutoTokenizer.from_pretrained(prediction_model_path, use_fast=True)\n",
    "prediction_model = AutoModelForSeq2SeqLM.from_pretrained(prediction_model_path)\n",
    "prediction_model = prediction_model.to(device)\n",
    "prediction_t = prediction_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTIONS TO PREDICT THE LABELS FOR CD2\n",
    "\n",
    "import re\n",
    "\n",
    "def find_true_pred(pred, labels):\n",
    "    match = []\n",
    "    for x in pred:\n",
    "        flag = False\n",
    "        idx = 0\n",
    "        while idx < len(labels) and not flag:\n",
    "            label = labels[idx]\n",
    "            if x in label or label in x or x == label:\n",
    "                match.append(label)\n",
    "                flag = True\n",
    "            \n",
    "            idx += 1\n",
    "    \n",
    "    if not flag:\n",
    "        print('NO')\n",
    "        \n",
    "    return match\n",
    "\n",
    "\n",
    "def get_labels(preds, labels):\n",
    "    preds = [x.split('+') if '+' in x else x.split(', ') for x in preds]\n",
    "\n",
    "    clean_preds = []\n",
    "    c =0\n",
    "\n",
    "    for pred in preds:\n",
    "        x = find_true_pred(pred, labels)\n",
    "\n",
    "        if len(x) <1:\n",
    "            #print(pred)\n",
    "            c += 1\n",
    "        clean_preds.append(x)\n",
    "\n",
    "    \n",
    "    print(\"NUMBR OF EMPTY PREDS : \", c)\n",
    "                \n",
    "    return clean_preds\n",
    "\n",
    "\n",
    "def generate_summary_k(test_samples):\n",
    "    samples = test_samples\n",
    "    generated_output_str = []\n",
    "\n",
    "    for i in tqdm(range(len(samples))):\n",
    "        inputs = prediction_tokenizer(\n",
    "            samples[i],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=encoder_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = inputs.input_ids.to(model.device)\n",
    "        attention_mask = inputs.attention_mask.to(model.device)\n",
    "        outputs = prediction_model.generate(input_ids, attention_mask=attention_mask)\n",
    "        output_str = prediction_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        output_str = output_str[0].replace('+', ', ')\n",
    "        generated_output_str.append(output_str)\n",
    "        \n",
    "    return generated_output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET OR MAKE THE REFERENCE LABELS FILE\n",
    "\n",
    "#If the labels expected for CD2 have already been generated by BART NO-CD, set to False\n",
    "make_references = True\n",
    "\n",
    "labels_path = output_path + 'reference_labels_filter_rerank_window3.csv'\n",
    "\n",
    "\n",
    "#IF MAKE REFERENCES, BART NO-CD WILL GENERATE THE SEQUENCES OF LABELS FOR ALL THE TEST SAMPLES\n",
    "if make_references:\n",
    "    acts = ['inform', 'question', 'directive', 'commissive']\n",
    "    emos= ['neutral', 'anger', 'disgust', 'fear', 'happiness', 'sadness','surprise']\n",
    "    labels_list = acts + emos\n",
    "\n",
    "    #Get Labels from dataset\n",
    "    labels = pd.read_csv('Labels Prediction/data/daily_dialog_test.csv', encoding = 'UTF-8')\n",
    "    ref = pd.read_csv(references_path_dialoGPT, encoding = 'UTF-8')\n",
    "\n",
    "    text = labels['text'].tolist()\n",
    "    labels = labels['label'].tolist()\n",
    "\n",
    "    df_labels_expected = []\n",
    "    for i, row in tqdm(ref.iterrows()):\n",
    "        flag = True\n",
    "        j =0\n",
    "        while j < len(text) and flag:\n",
    "            if row['actual responses'] == text[j]:\n",
    "                df_labels_expected.append([labels[j]])\n",
    "                flag= False\n",
    "            j +=1\n",
    "\n",
    "        if flag == True:\n",
    "            print('Interesting...')\n",
    "\n",
    "    contexts = df['inputs']\n",
    "    contexts = [ast.literal_eval(c)[-1] for c in contexts]\n",
    "    contexts = [n.strip() for n in contexts]\n",
    "\n",
    "    df_labels_predicted = generate_summary_k(contexts)\n",
    "    df_labels_predicted = get_labels(df_labels_predicted, labels_list)\n",
    "\n",
    "    #SAVE LABELS\n",
    "    if len(df_labels_expected) == len(df) and len(df_labels_predicted) == len(df):\n",
    "        print('Saving Labels...')\n",
    "        df_labels = pd.DataFrame({'inputs':ref['actual responses'], 'ground_truth': df_labels_expected, 'predicted': df_labels_predicted})\n",
    "        df_labels.to_csv(labels_path, encoding='UTF-8', index=False)\n",
    "\n",
    "    else:\n",
    "        print('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEN THE LABELS FILE \n",
    "\n",
    "labels_ref = pd.read_csv(labels_path, encoding='UTF-8')\n",
    "labels_ref.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> FILTER & RERANK LOOP</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT LABELS OF CURRENT CANDIDATE USING BERT CURRENT\n",
    "\n",
    "def predict_sentence(text, k=None, verbose=False):\n",
    "    features = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    features = features.to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**features)\n",
    "        logits = outputs[0]\n",
    "        logits = logits.sigmoid()\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    # sort results by descending order\n",
    "    pred_scores = np.sort(logits)[:, ::-1]\n",
    "    pred_ids = np.argsort(logits)[:, ::-1]\n",
    "\n",
    "    pred_scores = pred_scores[0]\n",
    "    pred_labels = id2label[pred_ids[0]]\n",
    "    \n",
    "    if k is not None:\n",
    "        pred_scores = pred_scores[:k]\n",
    "        pred_labels = pred_labels[:k]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'\"{text}\"')\n",
    "        for i, (s, l) in enumerate(zip(pred_scores, pred_labels)):\n",
    "            print(f\"{l:30} : {s}\")\n",
    "        print()\n",
    "    \n",
    "    return pred_labels, pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE NORMALISED LEVENSHTEIN AS THE SIMILARITY METRIC\n",
    "\n",
    "from textdistance import levenshtein\n",
    "\n",
    "def similarity(reference, results):\n",
    "    return levenshtein.normalized_similarity(results,reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTIONS\n",
    "\n",
    "\n",
    "#GET first N candidates\n",
    "def filter(candidates, k=10):\n",
    "    topk= candidates[:k]\n",
    "    return topk\n",
    "\n",
    "\n",
    "#Select only the predictions that are above the fixed threshold\n",
    "def select_pred(predictions, scores, threshold):\n",
    "    selected = [predictions[x] for x in range(len(predictions)) if scores[x] >= threshold]\n",
    "    return selected\n",
    "\n",
    "\n",
    "#Get similarity scores between the labels of the candidates and the labels expected\n",
    "def get_scores(candidates, reference):\n",
    "    similarity_scores = []\n",
    "    for candidate in candidates:\n",
    "        results, scores = predict_sentence(candidate)\n",
    "        results = select_pred(results, scores, 0.7)\n",
    "        if len(results) < 1:\n",
    "            results = select_pred(results, scores, 0.5)\n",
    "\n",
    "        #print(results, reference)\n",
    "        similarity_scores.append(similarity(reference, results))\n",
    "\n",
    "\n",
    "    return similarity_scores\n",
    "\n",
    "\n",
    "#Rank the list of candidates according to their similarity scores\n",
    "def rerank(candidates, scores):\n",
    "    ranked_scores = scores\n",
    "    ranked_scores.sort(reverse=True)\n",
    "    ranked_candidates = []\n",
    "    #print(ranked_scores)\n",
    "\n",
    "    for score in ranked_scores:\n",
    "        idx = scores.index(score)\n",
    "        ranked_candidates.append(candidates[idx])\n",
    "    return ranked_candidates, ranked_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the reference file, gets the candidates response associated with each test sample\n",
    "def get_candidates(df, columns):\n",
    "    candidates = []\n",
    "    for i in range(len(df)):\n",
    "        candidate = [df[j][i] for j in columns]\n",
    "        candidates.append([c for c in candidate if isinstance(c, str)])\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE FILTER AND RERANK FUNCTION FOR ONE REFERENCE FILE\n",
    "\n",
    "def filter_rerank(df):\n",
    "\n",
    "    #GET RESPONSES, INPUTS AND LABELS\n",
    "    references = df['actual responses']\n",
    "    inputs = df['inputs']\n",
    "    labels = df['labels']\n",
    "\n",
    "    #ONLY KEEP CANDIDATES, AND LABELS\n",
    "    columns = [x for x in df.columns if x not in [\"actual responses\", 'inputs']]\n",
    "    print(columns)\n",
    "\n",
    "    #GET CANDIDATES\n",
    "    candidates = get_candidates(df, columns)\n",
    "    #print(candidates[0])\n",
    "\n",
    "    #INITIALISE RESULTS DICT\n",
    "    results = {'input':[], 'reference': [], 'prediction': [], 'similarity': []}\n",
    "\n",
    "    #FOR EACH TEST SAMPLE, SELECT THE FINAL CANDIDATE\n",
    "    for i in tqdm(range(len(df))):\n",
    "        current_candidates = filter(candidates[i])\n",
    "        current_reference = references[i]\n",
    "        current_labels = labels[i]\n",
    "        current_input = inputs[i]\n",
    "\n",
    "        #FOR EACH CANDIDATE, GET THEIR LABELS AND THEIR SCORES TO COMPUTE SIMILARITY\n",
    "        current_scores = get_scores(current_candidates, current_labels)\n",
    "\n",
    "        #RERANK THE CANDIDATES WITH THEIR SIMILARITY SCORES\n",
    "        if len(current_scores) > 0:\n",
    "            ranked_candidates, ranked_scores = rerank(current_candidates, current_scores)\n",
    "        else:\n",
    "            ranked_candidates, ranked_scores = current_candidates, current_scores\n",
    "\n",
    "        #ADD TO RESULTS DICT\n",
    "        results['input'].append(current_input)\n",
    "        results['reference'].append(current_reference)\n",
    "\n",
    "        if len(ranked_candidates) > 0:\n",
    "            results['prediction'].append(ranked_candidates[0])\n",
    "            results['similarity'].append(ranked_scores[0])\n",
    "            \n",
    "        else:\n",
    "            results['prediction'].append('None')\n",
    "            results['similarity'].append(0.0)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOOP THE PROCESS ON ALL CONFIGURATIONS OF MODEL / CD1/2\n",
    "\n",
    "for model_name, reference_path in models_list.items():\n",
    "    print(model_name, reference_path) # FOR EACH SET OF RESULT\n",
    "\n",
    "    for mode in modes:\n",
    "        print('CURRENT MODE: ', mode)\n",
    "        df = pd.read_csv(reference_path, encoding = 'UTF-8')\n",
    "\n",
    "        if mode == \"CD1\":\n",
    "            df['labels'] = [ast.literal_eval(x) for x in labels_ref['ground_truth']]\n",
    "            \n",
    "        else:\n",
    "            df['labels'] = [ast.literal_eval(x) for x in labels_ref['predicted']]\n",
    "\n",
    "        print('Computing results...')\n",
    "\n",
    "        #CALL FILTER AND RERANK FUNCTION\n",
    "        results = filter_rerank(df)\n",
    "\n",
    "        #SAVE FINAL CANDIDATES\n",
    "        output_path = '/Filter Rerank/results_filter_window3/'\n",
    "        file_generated = output_path + model_name + \"_final_responses_window3_\" + mode\n",
    "        print(file_generated)\n",
    "\n",
    "        print('Saving results...')\n",
    "        df_new = pd.DataFrame(results)\n",
    "        df_new.to_csv(file_generated +'.csv', index = False, encoding = 'UTF-8')\n",
    "\n",
    "\n",
    "        generated_responses =results['reference']\n",
    "        actual_responses= results['prediction']\n",
    "\n",
    "        #GET METRICS\n",
    "        print('Computing scores...')\n",
    "        bleu_score = sacrebleu.compute(predictions=generated_responses, references=actual_responses)\n",
    "\n",
    "        rouge_score = rouge.compute(predictions=generated_responses, references=actual_responses)\n",
    "\n",
    "        bert_score = bertscore.compute(predictions=generated_responses, references=actual_responses, lang='en')\n",
    "        precision = bert_score['precision']\n",
    "        recall = bert_score['recall']\n",
    "        f1 = bert_score['f1']\n",
    "        avg_precision_bert = sum(precision) / len(precision)\n",
    "        avg_recall_bert = sum(recall) / len(recall)\n",
    "        avg_f1_bert = sum(f1) / len(f1)\n",
    "\n",
    "        chrf_score = chrf.compute(predictions=generated_responses, references=actual_responses)    \n",
    "\n",
    "        #COSINE SIMILARITY\n",
    "        embedding_1= cosine_model.encode(generated_responses)\n",
    "        embedding_2 = cosine_model.encode(actual_responses)\n",
    "\n",
    "        cosine = [util.pytorch_cos_sim(embedding_1[i], embedding_2[i]) for i in range(len(generated_responses))]\n",
    "\n",
    "        new_cosine = []\n",
    "\n",
    "        for x in cosine:\n",
    "            new_cosine.append(x[0].item())\n",
    "\n",
    "        mean_cosine = mean(new_cosine)\n",
    "        \n",
    "        #WRITE IN TXT FILE\n",
    "        print('Saving results...')\n",
    "        fout = open(file_generated+\".txt\", \"w\")\n",
    "        fout.write('Bleu score: {} \\n '.format(bleu_score)) #Range from 0 to 100\n",
    "        fout.write('Rouge score: {} \\n'.format(rouge_score))\n",
    "        fout.write('Bert score:  {} \\n'.format(bert_score))\n",
    "        fout.write('Avg precision Bert score: {} \\n'.format(avg_precision_bert))\n",
    "        fout.write('Avg recall Bert score: {} \\n'.format(avg_recall_bert))\n",
    "        fout.write('Avg f1 Bert score: {} \\n'.format(avg_f1_bert))\n",
    "        fout.write('chrf score: {} \\n'.format(chrf_score))\n",
    "        fout.write('average similarity: {}\\n'.format(mean(results['similarity'])))\n",
    "        fout.write('average cosine similarity: {} \\n'.format(mean_cosine))\n",
    "        fout.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
