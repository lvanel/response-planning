{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Prompting Beluga (Llama2 model fine-tuned on Orca Dataset) on Response Generation in English</h1>\n",
    "\n",
    "<i>vers. 10/2023</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "window_size =3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD MODEL\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"   \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\")\n",
    "device = torch.device(0)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET DATA\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import ast\n",
    "\n",
    "data = load_dataset('daily_dialog')\n",
    "verbatims = data['test']['dialog']\n",
    "output_path = '/Response Generation/Beluga'\n",
    "labels_path = '/Filter Rerank/en/results_filter_reference_labels_filter_rerank_window3.csv'\n",
    "\n",
    "labels_ref = pd.read_csv(labels_path, encoding='UTF-8')\n",
    "labels_expected = [ast.literal_eval(x) for x in labels_ref['ground_truth']]\n",
    "labels_predicted = [ast.literal_eval(x) for x in labels_ref['predicted']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Beluga F&R</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE PROMPT\n",
    "# k allows to specify the expected number of candidates to generate (CD1 / CD2)\n",
    "# if k is None, generate one response: NO_CD\n",
    "\n",
    "def make_prompt(element, k = None):    \n",
    "    if k is None:\n",
    "        prompt = \"Generate the response following the given context. For example:\\nA: Do you like some soup ? B: Yes , but I don't know what soup you have A: We have beef soup and tomato soup  Response: Good . I prefer beef soup .\\nA: Can I take your order now , Madam ? B: Yes , what would you recommend ? A: I'm happy to recommend the fish , It testes delicious , and it is today's special . Our chef is from the coast , and love seafood . Today special is actually his favorite dish . so I'm sure it is a Response: It does sound wonderful , maybe I'll try it .\\n\\n Generate the response following the following dialogue: \" + element\n",
    "\n",
    "    else:\n",
    "\n",
    "        prompt = \"Generate \"+ str(k) + \" responses following this dialogue: \" + element +'\\nNumber the generated sequences from 1 to ' +str(k) +\" Generated sequences: 1: \"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "contexts = []\n",
    "responses = []\n",
    "responses_k = []\n",
    "\n",
    "for dialog in tqdm(verbatims):\n",
    "    for i in range(0, len(dialog) - window_size, 2): #In steps of 2\n",
    "\n",
    "        window = dialog[i:i+window_size]\n",
    "        contexts.append(window)\n",
    "        input = 'SPEAKER A: ' +  window[0]\n",
    "        current_speaker = 'B'\n",
    "\n",
    "        for utterance in window[1:]:\n",
    "            if current_speaker == 'A':\n",
    "                input += ' SPEAKER A: ' + utterance\n",
    "                current_speaker = 'B'\n",
    "            \n",
    "            else:\n",
    "                input += ' SPEAKER B: ' + utterance\n",
    "                current_speaker = 'A'\n",
    "\n",
    "        #NO-CD\n",
    "        p = make_prompt(input)\n",
    "        inputs = tokenizer(p, return_tensors=\"pt\").to(device)\n",
    "        output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        response = response[len(p):]\n",
    "        responses.append(response)\n",
    "\n",
    "        #MULTIPLE RESPONSES FOR CD1 / CD2\n",
    "        p_k = make_prompt(input, k=10)\n",
    "        inputs = tokenizer(p_k, return_tensors=\"pt\").to(device)\n",
    "        output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "        response_k = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        response_k = response_k[len(p_k):]\n",
    "\n",
    "        responses_k.append(response_k)\n",
    "\n",
    "print(responses_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE RESULTS\n",
    "\n",
    "df = pd.DataFrame({'input': contexts, 'hypothese': responses}).to_csv(output_path + 'beluga_labels_fr.csv', encoding = 'UTF-8', index=False)\n",
    "df = pd.DataFrame({'input': contexts, 'hypothese': responses_k}).to_csv(output_path + 'beluga_labels_fr_k_10.csv', encoding = 'UTF-8', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Beluga PB</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(element, labels):    \n",
    "    prompt = \"Generate the response following the given context :\" +  element + \"\\n The tone of the response must be \" + labels + \"\\nResponse: \"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "contexts = []\n",
    "responses_exp = []\n",
    "responses_pred = []\n",
    "\n",
    "for dialog in tqdm(verbatims):\n",
    "    for i in range(0, len(dialog) - window_size, 2): #In steps of 2\n",
    "\n",
    "        window = dialog[i:i+window_size]\n",
    "        contexts.append(window)\n",
    "        input = 'SPEAKER A: ' +  window[0]\n",
    "        current_speaker = 'B'\n",
    "\n",
    "        for utterance in window[1:]:\n",
    "            if current_speaker == 'A':\n",
    "                input += ' SPEAKER A: ' + utterance\n",
    "                current_speaker = 'B'\n",
    "            \n",
    "            else:\n",
    "                input += ' SPEAKER B: ' + utterance\n",
    "                current_speaker = 'A'\n",
    "\n",
    "        #CD1: GENERATE RESPONSE USING DATASET EXPECTED LABELS\n",
    "        p_exp = make_prompt(input, ', '.join(labels_expected[i]))\n",
    "        inputs_exp = tokenizer(p_exp, return_tensors=\"pt\").to(device)\n",
    "        output_exp = model.generate(**inputs_exp, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "        response_exp = tokenizer.decode(output_exp[0], skip_special_tokens=True)\n",
    "        response_exp = response_exp[len(p_exp):]\n",
    "        responses_exp.append(response_exp)\n",
    "\n",
    "\n",
    "        #CD2: GENERATE RESPONSE USING BART-GENERATED EXPECTED LABELS\n",
    "        p_pred = make_prompt(input, ', '.join(labels_predicted[i]))\n",
    "        inputs_pred = tokenizer(p_pred, return_tensors=\"pt\").to(device)\n",
    "        output_pred = model.generate(**inputs_pred, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "        response_pred = tokenizer.decode(output_pred[0], skip_special_tokens=True)\n",
    "        response_pred = response_pred[len(p_pred):]\n",
    "\n",
    "        responses_pred.append(response_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE RESULTS\n",
    "\n",
    "df = pd.DataFrame({'input': contexts, 'hypothese': responses_exp}).to_csv(output_path + 'beluga_pb_expected_window3.csv', encoding = 'UTF-8', index=False)\n",
    "df = pd.DataFrame({'input': contexts, 'hypothese': responses_pred}).to_csv(output_path + 'beluga_pb_pred_window3.csv', encoding = 'UTF-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
